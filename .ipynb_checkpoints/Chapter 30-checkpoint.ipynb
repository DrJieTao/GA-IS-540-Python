{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (c) 2014 Reid Johnson\n",
    "#\n",
    "# Modified from:\n",
    "# sklearn (https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/weight_boosting.py)\n",
    "#\n",
    "# Generates a \"boosted\" ensemble of base models.\n",
    "\n",
    "import copy\n",
    "import operator\n",
    "import sys, math, random\n",
    "import numpy\n",
    "\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class AdaBoostClassifier(ClassifierMixin):\n",
    "    \"\"\"An AdaBoost classifier.\n",
    "\n",
    "    An AdaBoost [1] classifier is a meta-estimator that begins by fitting a\n",
    "    classifier on the original dataset and then fits additional copies of the\n",
    "    classifier on the same dataset but where the weights of incorrectly\n",
    "    classified instances are adjusted such that subsequent classifiers focus\n",
    "    more on difficult cases.\n",
    "\n",
    "    This class implements the algorithm known as AdaBoost-SAMME [2].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimator : object, optional (default=DecisionTreeClassifier)\n",
    "        The base estimator from which the boosted ensemble is built.\n",
    "        Support for sample weighting is required, as well as proper `classes_`\n",
    "        and `n_classes_` attributes.\n",
    "\n",
    "    n_estimators : integer, optional (default=50)\n",
    "        The maximum number of estimators at which boosting is terminated.\n",
    "        In case of perfect fit, the learning procedure is stopped early.\n",
    "\n",
    "    learning_rate : float, optional (default=1.)\n",
    "        Learning rate shrinks the contribution of each classifier by\n",
    "        ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
    "        ``n_estimators``.\n",
    "\n",
    "    algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')\n",
    "        If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n",
    "        ``base_estimator`` must support calculation of class probabilities.\n",
    "        If 'SAMME' then use the SAMME discrete boosting algorithm.\n",
    "        The SAMME.R algorithm typically converges faster than SAMME,\n",
    "        achieving a lower test error with fewer boosting iterations.\n",
    "\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    `estimators_` : list of classifiers\n",
    "        The collection of fitted sub-estimators.\n",
    "\n",
    "    `classes_` : array of shape = [n_classes]\n",
    "        The classes labels.\n",
    "\n",
    "    `n_classes_` : int\n",
    "        The number of classes.\n",
    "\n",
    "    `estimator_weights_` : array of floats\n",
    "        Weights for each estimator in the boosted ensemble.\n",
    "\n",
    "    `estimator_errors_` : array of floats\n",
    "        Classification error for each estimator in the boosted\n",
    "        ensemble.\n",
    "\n",
    "    `feature_importances_` : array of shape = [n_features]\n",
    "        The feature importances if supported by the ``base_estimator``.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n",
    "           on-Line Learning and an Application to Boosting\", 1995.\n",
    "\n",
    "    .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "                 n_estimators=50,\n",
    "                 estimator_params=tuple(),\n",
    "                 learning_rate=1.,\n",
    "                 algorithm='SAMME.R'):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.estimator_params = estimator_params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.algorithm = algorithm\n",
    "\n",
    "    def _make_estimator(self, append=True):\n",
    "        \"\"\"Make and configure a copy of the `base_estimator_` attribute.\n",
    "\n",
    "        Warning: This method should be used to properly instantiate new\n",
    "        sub-estimators.\n",
    "        \"\"\"\n",
    "        estimator = copy.deepcopy(self.base_estimator)\n",
    "        estimator.set_params(**dict((p, getattr(self, p))\n",
    "                                    for p in self.estimator_params))\n",
    "\n",
    "        if append:\n",
    "            self.estimators_.append(estimator)\n",
    "\n",
    "        return estimator\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        # Check parameters.\n",
    "        if self.learning_rate <= 0:\n",
    "            raise ValueError(\"learning_rate must be greater than zero.\")\n",
    "\n",
    "        if sample_weight is None:\n",
    "            # Initialize weights to 1 / n_samples.\n",
    "            sample_weight = np.empty(X.shape[0], dtype=np.float)\n",
    "            sample_weight[:] = 1. / X.shape[0]\n",
    "        else:\n",
    "            # Normalize existing weights.\n",
    "            sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)\n",
    "\n",
    "        # Check that the sample weights sum is positive.\n",
    "        if sample_weight.sum() <= 0:\n",
    "            raise ValueError(\n",
    "                \"Attempting to fit with a non-positive \"\n",
    "                \"weighted number of samples.\")\n",
    "\n",
    "        # Clear any previous fit results.\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float)\n",
    "        self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float)\n",
    "\n",
    "        for iboost in range(self.n_estimators):\n",
    "            #print 'Iteration [%s]' % (iboost)\n",
    "\n",
    "            # Fit the estimator.\n",
    "            estimator = self._make_estimator()\n",
    "            estimator.fit(X, y, sample_weight=sample_weight)\n",
    "\n",
    "            if iboost == 0:\n",
    "                self.classes_ = getattr(estimator, 'classes_', None)\n",
    "                self.n_classes_ = len(self.classes_)\n",
    "\n",
    "            # Generate estimator predictions.\n",
    "            y_pred = estimator.predict(X)\n",
    "\n",
    "            # Instances incorrectly classified.\n",
    "            incorrect = y_pred != y\n",
    "\n",
    "            # Error fraction.\n",
    "            estimator_error = np.mean(\n",
    "                np.average(incorrect, weights=sample_weight, axis=0))\n",
    "\n",
    "            # Boost weight using multi-class AdaBoost SAMME alg.\n",
    "            estimator_weight = self.learning_rate * (\n",
    "                np.log((1. - estimator_error) / estimator_error) +\n",
    "                np.log(self.n_classes_ - 1.))\n",
    "\n",
    "            # Only boost the weights if there is another iteration of fitting.\n",
    "            if not iboost == self.n_estimators - 1:\n",
    "                # Only boost positive weights (exponential loss).\n",
    "                sample_weight *= np.exp(estimator_weight * incorrect *\n",
    "                                        ((sample_weight > 0) |\n",
    "                                         (estimator_weight < 0)))\n",
    "\n",
    "            self.estimator_weights_[iboost] = estimator_weight\n",
    "            self.estimator_errors_[iboost] = estimator_error\n",
    "\n",
    "    def _check_fitted(self):\n",
    "        if not hasattr(self, \"estimators_\"):\n",
    "            raise ValueError(\"Call 'fit' first.\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = numpy.array(X)\n",
    "        N, d = X.shape\n",
    "        pred = numpy.zeros(N)\n",
    "        for estimator, w in zip(self.estimators_, self.estimator_weights_):\n",
    "            pred += estimator.predict(X) * w\n",
    "        pred /= self.estimator_weights_.sum()\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Compute the decision function of ``X``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape = [n_samples, n_features]\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        score : array, shape = [n_samples, k]\n",
    "            The decision function of the input samples. The order of\n",
    "            outputs is the same of that of the `classes_` attribute.\n",
    "            Binary classification is a special cases with ``k == 1``,\n",
    "            otherwise ``k==n_classes``. For binary classification,\n",
    "            values closer to -1 or 1 mean more like the first or second\n",
    "            class in ``classes_``, respectively.\n",
    "        \"\"\"\n",
    "        self._check_fitted()\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        n_classes = self.n_classes_\n",
    "        classes = self.classes_[:, np.newaxis]\n",
    "        pred = None\n",
    "\n",
    "        if self.algorithm == 'SAMME.R':\n",
    "            # The weights are all 1. for SAMME.R\n",
    "            pred = sum(_samme_proba(estimator, n_classes, X)\n",
    "                       for estimator in self.estimators_)\n",
    "        else:   # self.algorithm == \"SAMME\"\n",
    "            pred = sum((estimator.predict(X) == classes).T * w\n",
    "                       for estimator, w in zip(self.estimators_,\n",
    "                                               self.estimator_weights_))\n",
    "\n",
    "        pred /= self.estimator_weights_.sum()\n",
    "        if n_classes == 2:\n",
    "            pred[:, 0] *= -1\n",
    "            return pred.sum(axis=1)\n",
    "        return pred\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes for X.\n",
    "\n",
    "        The predicted class of an input sample is computed as the weighted mean\n",
    "        prediction of the classifiers in the ensemble.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape = [n_samples, n_features]\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array of shape = [n_samples]\n",
    "            The predicted classes.\n",
    "        \"\"\"\n",
    "        pred = self.decision_function(X)\n",
    "\n",
    "        if self.n_classes_ == 2:\n",
    "            return self.classes_.take(pred > 0, axis=0)\n",
    "\n",
    "        return self.classes_.take(np.argmax(pred, axis=1), axis=0)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities for X.\n",
    "\n",
    "        The predicted class probabilities of an input sample is computed as\n",
    "        the weighted mean predicted class probabilities of the classifiers\n",
    "        in the ensemble.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape = [n_samples, n_features]\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples]\n",
    "            The class probabilities of the input samples. The order of\n",
    "            outputs is the same of that of the `classes_` attribute.\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        n_classes = self.n_classes_\n",
    "\n",
    "        if self.algorithm == 'SAMME.R':\n",
    "            # The weights are all 1. for SAMME.R\n",
    "            proba = sum(_samme_proba(estimator, n_classes, X)\n",
    "                        for estimator in self.estimators_)\n",
    "        else:   # self.algorithm == \"SAMME\"\n",
    "            proba = sum(estimator.predict_proba(X) * w\n",
    "                        for estimator, w in zip(self.estimators_,\n",
    "                                                self.estimator_weights_))\n",
    "\n",
    "        proba /= self.estimator_weights_.sum()\n",
    "        proba = np.exp((1. / (n_classes - 1)) * proba)\n",
    "        normalizer = proba.sum(axis=1)[:, np.newaxis]\n",
    "        normalizer[normalizer == 0.0] = 1.0\n",
    "        proba /= normalizer\n",
    "\n",
    "        return proba\n",
    "\n",
    "def _samme_proba(estimator, n_classes, X):\n",
    "    \"\"\"Calculate algorithm 4, step 2, equation c) of Zhu et al [1].\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n",
    "\n",
    "    \"\"\"\n",
    "    proba = estimator.predict_proba(X)\n",
    "\n",
    "    # Displace zero probabilities so the log is defined.\n",
    "    # Also fix negative elements which may occur with\n",
    "    # negative sample weights.\n",
    "    proba[proba <= 0] = 1e-5\n",
    "    log_proba = np.log(proba)\n",
    "\n",
    "    return (n_classes - 1) * (log_proba - (1. / n_classes)\n",
    "                           * log_proba.sum(axis=1)[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import operator\n",
    "import sys, math, random\n",
    "import numpy\n",
    "\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class LogitBoostClassifier(ClassifierMixin):\n",
    "    \"\"\"A LogitBoost classifier.\n",
    "\n",
    "    A LogitBoost [1] classifier is a meta-estimator that begins by fitting a\n",
    "    classifier on the original dataset and then fits additional copies of the\n",
    "    classifier on the same dataset but where the weights of incorrectly\n",
    "    classified instances are adjusted such that subsequent classifiers focus\n",
    "    more on difficult cases.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimator : object, optional (default=DecisionTreeClassifier)\n",
    "        The base estimator from which the boosted ensemble is built.\n",
    "        Support for sample weighting is required, as well as proper `classes_`\n",
    "        and `n_classes_` attributes.\n",
    "\n",
    "    n_estimators : integer, optional (default=50)\n",
    "        The maximum number of estimators at which boosting is terminated.\n",
    "        In case of perfect fit, the learning procedure is stopped early.\n",
    "\n",
    "    learning_rate : float, optional (default=1.)\n",
    "        Learning rate shrinks the contribution of each classifier by\n",
    "        ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
    "        ``n_estimators``.\n",
    "\n",
    "    algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')\n",
    "        If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n",
    "        ``base_estimator`` must support calculation of class probabilities.\n",
    "        If 'SAMME' then use the SAMME discrete boosting algorithm.\n",
    "        The SAMME.R algorithm typically converges faster than SAMME,\n",
    "        achieving a lower test error with fewer boosting iterations.\n",
    "\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    `estimators_` : list of classifiers\n",
    "        The collection of fitted sub-estimators.\n",
    "\n",
    "    `classes_` : array of shape = [n_classes]\n",
    "        The classes labels.\n",
    "\n",
    "    `n_classes_` : int\n",
    "        The number of classes.\n",
    "\n",
    "    `estimator_weights_` : array of floats\n",
    "        Weights for each estimator in the boosted ensemble.\n",
    "\n",
    "    `estimator_errors_` : array of floats\n",
    "        Classification error for each estimator in the boosted\n",
    "        ensemble.\n",
    "\n",
    "    `feature_importances_` : array of shape = [n_features]\n",
    "        The feature importances if supported by the ``base_estimator``.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] J. Friedman, T. Hastie, R. Tibshirani, \"Additive Logistic Regression: \n",
    "           A Statistical View of Boosting\", 2000.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "                 n_estimators=50,\n",
    "                 estimator_params=tuple(),\n",
    "                 learning_rate=1.,\n",
    "                 algorithm='SAMME.R'):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.estimator_params = estimator_params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.algorithm = algorithm\n",
    "\n",
    "    def _make_estimator(self, append=True):\n",
    "        \"\"\"Make and configure a copy of the `base_estimator_` attribute.\n",
    "\n",
    "        Warning: This method should be used to properly instantiate new\n",
    "        sub-estimators.\n",
    "        \"\"\"\n",
    "        estimator = copy.deepcopy(self.base_estimator)\n",
    "        estimator.set_params(**dict((p, getattr(self, p))\n",
    "                                    for p in self.estimator_params))\n",
    "\n",
    "        if append:\n",
    "            self.estimators_.append(estimator)\n",
    "\n",
    "        return estimator\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        # Check parameters.\n",
    "        if self.learning_rate <= 0:\n",
    "            raise ValueError(\"learning_rate must be greater than zero.\")\n",
    "\n",
    "        if sample_weight is None:\n",
    "            # Initialize weights to 1 / n_samples.\n",
    "            sample_weight = np.empty(X.shape[0], dtype=np.float)\n",
    "            sample_weight[:] = 1. / X.shape[0]\n",
    "        else:\n",
    "            # Normalize existing weights.\n",
    "            sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)\n",
    "\n",
    "        # Check that the sample weights sum is positive.\n",
    "        if sample_weight.sum() <= 0:\n",
    "            raise ValueError(\n",
    "                \"Attempting to fit with a non-positive \"\n",
    "                \"weighted number of samples.\")\n",
    "\n",
    "        # Clear any previous fit results.\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float)\n",
    "        self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float)\n",
    "\n",
    "        for iboost in range(self.n_estimators):\n",
    "            #print 'Iteration [%s]' % (iboost)\n",
    "\n",
    "            # Fit the estimator.\n",
    "            estimator = self._make_estimator()\n",
    "            estimator.fit(X, y, sample_weight=sample_weight)\n",
    "\n",
    "            if iboost == 0:\n",
    "                self.classes_ = getattr(estimator, 'classes_', None)\n",
    "                self.n_classes_ = len(self.classes_)\n",
    "\n",
    "            # Generate estimator predictions.\n",
    "            y_pred = estimator.predict(X)\n",
    "\n",
    "            # Instances incorrectly classified.\n",
    "            incorrect = y_pred != y\n",
    "\n",
    "            # Error fraction.\n",
    "            estimator_error = np.mean(\n",
    "                np.average(incorrect, weights=sample_weight, axis=0))\n",
    "\n",
    "            # Boost weight using multi-class AdaBoost SAMME alg.\n",
    "            estimator_weight = self.learning_rate * (\n",
    "                np.log((1. - estimator_error) / estimator_error) +\n",
    "                np.log(self.n_classes_ - 1.))\n",
    "\n",
    "            # Only boost the weights if there is another iteration of fitting.\n",
    "            if not iboost == self.n_estimators - 1:\n",
    "                # Only boost positive weights (logistic loss).\n",
    "                sample_weight *= np.log(1 + np.exp(estimator_weight * incorrect *\n",
    "                                        ((sample_weight > 0) |\n",
    "                                         (estimator_weight < 0))))\n",
    "\n",
    "            self.estimator_weights_[iboost] = estimator_weight\n",
    "            self.estimator_errors_[iboost] = estimator_error\n",
    "\n",
    "    def _check_fitted(self):\n",
    "        if not hasattr(self, \"estimators_\"):\n",
    "            raise ValueError(\"Call 'fit' first.\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = numpy.array(X)\n",
    "        N, d = X.shape\n",
    "        pred = numpy.zeros(N)\n",
    "        for estimator, w in zip(self.estimators_, self.estimator_weights_):\n",
    "            pred += estimator.predict(X) * w\n",
    "        pred /= self.estimator_weights_.sum()\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Compute the decision function of ``X``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape = [n_samples, n_features]\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        score : array, shape = [n_samples, k]\n",
    "            The decision function of the input samples. The order of\n",
    "            outputs is the same of that of the `classes_` attribute.\n",
    "            Binary classification is a special cases with ``k == 1``,\n",
    "            otherwise ``k==n_classes``. For binary classification,\n",
    "            values closer to -1 or 1 mean more like the first or second\n",
    "            class in ``classes_``, respectively.\n",
    "        \"\"\"\n",
    "        self._check_fitted()\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        n_classes = self.n_classes_\n",
    "        classes = self.classes_[:, np.newaxis]\n",
    "        pred = None\n",
    "\n",
    "        if self.algorithm == 'SAMME.R':\n",
    "            # The weights are all 1. for SAMME.R\n",
    "            pred = sum(_samme_proba(estimator, n_classes, X)\n",
    "                       for estimator in self.estimators_)\n",
    "        else:   # self.algorithm == \"SAMME\"\n",
    "            pred = sum((estimator.predict(X) == classes).T * w\n",
    "                       for estimator, w in zip(self.estimators_,\n",
    "                                               self.estimator_weights_))\n",
    "\n",
    "        pred /= self.estimator_weights_.sum()\n",
    "        if n_classes == 2:\n",
    "            pred[:, 0] *= -1\n",
    "            return pred.sum(axis=1)\n",
    "        return pred\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes for X.\n",
    "\n",
    "        The predicted class of an input sample is computed as the weighted mean\n",
    "        prediction of the classifiers in the ensemble.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape = [n_samples, n_features]\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array of shape = [n_samples]\n",
    "            The predicted classes.\n",
    "        \"\"\"\n",
    "        pred = self.decision_function(X)\n",
    "\n",
    "        if self.n_classes_ == 2:\n",
    "            return self.classes_.take(pred > 0, axis=0)\n",
    "\n",
    "        return self.classes_.take(np.argmax(pred, axis=1), axis=0)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities for X.\n",
    "\n",
    "        The predicted class probabilities of an input sample is computed as\n",
    "        the weighted mean predicted class probabilities of the classifiers\n",
    "        in the ensemble.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape = [n_samples, n_features]\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples]\n",
    "            The class probabilities of the input samples. The order of\n",
    "            outputs is the same of that of the `classes_` attribute.\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        n_classes = self.n_classes_\n",
    "\n",
    "        if self.algorithm == 'SAMME.R':\n",
    "            # The weights are all 1. for SAMME.R\n",
    "            proba = sum(_samme_proba(estimator, n_classes, X)\n",
    "                        for estimator in self.estimators_)\n",
    "        else:   # self.algorithm == \"SAMME\"\n",
    "            proba = sum(estimator.predict_proba(X) * w\n",
    "                        for estimator, w in zip(self.estimators_,\n",
    "                                                self.estimator_weights_))\n",
    "\n",
    "        proba /= self.estimator_weights_.sum()\n",
    "        proba = np.log(1 + np.exp((1. / (n_classes - 1)) * proba))\n",
    "        normalizer = proba.sum(axis=1)[:, np.newaxis]\n",
    "        normalizer[normalizer == 0.0] = 1.0\n",
    "        proba /= normalizer\n",
    "\n",
    "        return proba\n",
    "\n",
    "def _samme_proba(estimator, n_classes, X):\n",
    "    \"\"\"Calculate algorithm 4, step 2, equation c) of Zhu et al [1].\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n",
    "\n",
    "    \"\"\"\n",
    "    proba = estimator.predict_proba(X)\n",
    "\n",
    "    # Displace zero probabilities so the log is defined.\n",
    "    # Also fix negative elements which may occur with\n",
    "    # negative sample weights.\n",
    "    proba[proba <= 0] = 1e-5\n",
    "    log_proba = np.log(proba)\n",
    "\n",
    "    return (n_classes - 1) * (log_proba - (1. / n_classes)\n",
    "                           * log_proba.sum(axis=1)[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "label_encode = True\n",
    "\n",
    "# Load the Iris flower dataset\n",
    "fileURL = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "iris = pd.read_csv(fileURL, \n",
    "                   names=['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width', 'Species'],\n",
    "                   header=None)\n",
    "iris = iris.dropna()\n",
    "\n",
    "X = iris[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']] # features\n",
    "labels = iris['Species'] # class\n",
    "\n",
    "if label_encode:\n",
    "    # Transform string (nominal) output to numeric\n",
    "    y = preprocessing.LabelEncoder().fit_transform(labels)\n",
    "else:\n",
    "    y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import cross_validation\n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "# The training sets will be used for all training and validation purposes.\n",
    "# The testing sets will only be used for evaluating the final blended (level 1) classifier.\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier(n_estimators=100)\n",
    "abc.fit(X_train, y_train)\n",
    "\n",
    "lbc = LogitBoostClassifier(n_estimators=100)\n",
    "lbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy = 0.977777777778\n",
      "LogitBoost Accuracy = 0.977777777778\n",
      "Base Classifier Accuracy = 0.966666666667\n",
      "sklearn AdaBoost Accuracy = 0.966666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "### Generate predictions with AdaBoost classifier. ###\n",
    "\n",
    "score = metrics.accuracy_score(y_test, abc.predict(X_test))\n",
    "print ('AdaBoost Accuracy = %s' % (score))\n",
    "print\n",
    "\n",
    "### Generate predictions with LogitBoost classifier. ###\n",
    "\n",
    "score = metrics.accuracy_score(y_test, lbc.predict(X_test))\n",
    "print ('LogitBoost Accuracy = %s' % (score))\n",
    "print\n",
    "\n",
    "### Generate predictions with base (non-boosted) classifier. ###\n",
    "\n",
    "clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "score = metrics.accuracy_score(y_test, clf.predict(X_test))\n",
    "print ('Base Classifier Accuracy = %s' % (score))\n",
    "print\n",
    "\n",
    "import sklearn.ensemble\n",
    "clf2 = sklearn.ensemble.AdaBoostClassifier(n_estimators=100).fit(X_train, y_train)\n",
    "score = metrics.accuracy_score(y_test, clf2.predict(X_test))\n",
    "print ('sklearn AdaBoost Accuracy = %s' % (score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
