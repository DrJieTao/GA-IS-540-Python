{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (c) 2014 Reid Johnson\n",
    "#\n",
    "# Modified from:\n",
    "# Daniel Rodriguez (http://danielfrg.com/blog/2013/07/03/basic-neural-network-python/)\n",
    "#\n",
    "# Functions to generate an artifical neural network (ANN) with one hidden \n",
    "# layer. The ANN is a collection of artificial neurons arranged in layers that \n",
    "# are trained using backpropogation.\n",
    "\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from sklearn.base import ClassifierMixin\n",
    "from __future__ import division\n",
    "\n",
    "class NN_1HL(ClassifierMixin):\n",
    "    \"\"\"Implements an artifical neural network (ANN) with one hidden layer.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] F. Rosenblatt, \"The Perceptron: A Probalistic Model for Information \n",
    "           Storage and Organization in the Brain,\" 1958. (Perceptron algorithm.)\n",
    "\n",
    "    .. [2] P. Werbos, \"Beyond Regression: New Tools for Prediction and Analysis \n",
    "           in the Behavioral Sciences,\" 1975. (Backpropogation algorithm.)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, reg_lambda=0, epsilon_init=0.12, hidden_layer_size=25, \n",
    "                 opti_method='TNC', maxiter=500):\n",
    "        self.reg_lambda = reg_lambda # weight for the logistic regression cost\n",
    "        self.epsilon_init = epsilon_init # learning rate\n",
    "        self.hidden_layer_size = hidden_layer_size # size of the hidden layer\n",
    "        self.activation_func = self.sigmoid # activation function\n",
    "        self.activation_func_prime = self.sigmoid_prime # derivative of the activation function\n",
    "        self.method = opti_method # optimization method\n",
    "        self.maxiter = maxiter # maximum number of iterations\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Returns the logistic function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_prime(self, z):\n",
    "        \"\"\"Returns the derivative of the logistic function.\"\"\"\n",
    "        sig = self.sigmoid(z)\n",
    "\n",
    "        return sig * (1-sig)\n",
    "\n",
    "    def sumsqr(self, a):\n",
    "        \"\"\"Returns the sum of squared values.\"\"\"\n",
    "        return np.sum(a**2)\n",
    "\n",
    "    def rand_init(self, l_in, l_out):\n",
    "        \"\"\"Generates an (l_out x l_in+1) array of thetas (threshold values), \n",
    "        each initialized to a random number between minus epsilon and epsilon.\n",
    "\n",
    "        Note that there is one theta matrix per layer. The size of each theta \n",
    "        matrix depends on the number of activation units in its corresponding \n",
    "        layer, so each matrix may be of a different size.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Randomly initialized thetas (threshold values).\n",
    "        \"\"\"\n",
    "        return np.random.rand(l_out, l_in+1) * 2 * self.epsilon_init - self.epsilon_init\n",
    "\n",
    "    # Pack thetas (threshold values) into a one-dimensional array.\n",
    "    def pack_thetas(self, t1, t2):\n",
    "        \"\"\"Packs (unrolls) thetas (threshold values) from matrices into a \n",
    "        single vector.\n",
    "\n",
    "        Note that there is one theta matrix per layer. To use an optimization \n",
    "        technique that minimizes the error, we need to pack (unroll) the \n",
    "        matrices into a single vector.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        t1 : array\n",
    "            Unpacked (rolled) thetas (threshold values) between the input \n",
    "            layer and hidden layer.\n",
    "\n",
    "        t2 : array\n",
    "            Unpacked (rolled) thetas (threshold values) between the hidden \n",
    "            layer and output layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Packed (unrolled) thetas (threshold values).\n",
    "        \"\"\"\n",
    "        return np.concatenate((t1.reshape(-1), t2.reshape(-1)))\n",
    "\n",
    "    def unpack_thetas(self, thetas, input_layer_size, hidden_layer_size, num_labels):\n",
    "        \"\"\"Unpacks (rolls) thetas (threshold values) from a single vector into \n",
    "        a multi-dimensional array (matrices).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        thetas : array\n",
    "            Packed (unrolled) thetas (threshold values).\n",
    "\n",
    "        input_layer_size : integer\n",
    "            Number of nodes in the input layer.\n",
    "\n",
    "        hidden_layer_size : integer\n",
    "            Number of nodes in the hidden layer.\n",
    "\n",
    "        num_labels : integer\n",
    "            Number of classes.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        t1 : array\n",
    "            Unpacked (rolled) thetas (threshold values) between the input \n",
    "            layer and hidden layer.\n",
    "\n",
    "        t2 : array\n",
    "            Unpacked (rolled) thetas (threshold values) between the hidden \n",
    "            layer and output layer.\n",
    "        \"\"\"\n",
    "        t1_start = 0\n",
    "        t1_end = hidden_layer_size * (input_layer_size + 1)\n",
    "        t1 = thetas[t1_start:t1_end].reshape((hidden_layer_size, input_layer_size + 1))\n",
    "        t2 = thetas[t1_end:].reshape((num_labels, hidden_layer_size + 1))\n",
    "\n",
    "        return t1, t2\n",
    "\n",
    "    def _forward(self, X, t1, t2):\n",
    "        \"\"\"Forward propogation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape=(n, m)\n",
    "            The feature data for which to compute the predicted output \n",
    "            probabilities.\n",
    "\n",
    "        t1 : array\n",
    "            Unpacked (rolled) thetas (threshold values) between the input \n",
    "            layer and hidden layer.\n",
    "\n",
    "        t2 : array\n",
    "            Unpacked (rolled) thetas (threshold values) between the hidden \n",
    "            layer and output layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a1 : array\n",
    "            The output activation of units in the input layer.\n",
    "\n",
    "        z2 : array\n",
    "            The input activation of units in the hidden layer.\n",
    "\n",
    "        a2 : array\n",
    "            The output activation of units in the hidden layer.\n",
    "\n",
    "        z3 : array\n",
    "            The input activation of units in the output layer.\n",
    "\n",
    "        a3 : array\n",
    "            The output activation of units in the output layer.\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        ones = None\n",
    "        if len(X.shape) == 1:\n",
    "            ones = np.array(1).reshape(1,)\n",
    "        else:\n",
    "            ones = np.ones(m).reshape(m,1)\n",
    "\n",
    "        # Input layer.\n",
    "        a1 = np.hstack((ones, X))\n",
    "\n",
    "        # Hidden layer.\n",
    "        z2 = np.dot(t1, a1.T)\n",
    "        a2 = self.activation_func(z2)\n",
    "        a2 = np.hstack((ones, a2.T))\n",
    "\n",
    "        # Output layer.\n",
    "        z3 = np.dot(t2, a2.T)\n",
    "        a3 = self.activation_func(z3)\n",
    "\n",
    "        return a1, z2, a2, z3, a3\n",
    "\n",
    "    def function(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):\n",
    "        \"\"\"Returns the total cost using a generalization of the regularized \n",
    "        logistic regression cost function.\n",
    "        \"\"\"\n",
    "        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)\n",
    "\n",
    "        m = X.shape[0]\n",
    "        Y = np.eye(num_labels)[y]\n",
    "\n",
    "        _, _, _, _, h = self._forward(X, t1, t2)\n",
    "        costPositive = -Y * np.log(h).T # the cost when y is 1\n",
    "        costNegative = (1 - Y) * np.log(1 - h).T # the cost when y is 0\n",
    "        cost = costPositive - costNegative # the total cost\n",
    "        J = np.sum(cost) / m # the (unregularized) cost function\n",
    "\n",
    "        # For regularization.\n",
    "        if reg_lambda != 0:\n",
    "            t1f = t1[:, 1:]\n",
    "            t2f = t2[:, 1:]\n",
    "            reg = (self.reg_lambda / (2*m)) * (self.sumsqr(t1f) + self.sumsqr(t2f)) # regularization term\n",
    "            J = J + reg # the regularized cost function\n",
    "\n",
    "        return J\n",
    "\n",
    "    def function_prime(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):\n",
    "        \"\"\"Returns the Jacobian matrix (the matrix of all first-order partial \n",
    "        derivatives) of the cost function.\n",
    "        \"\"\"\n",
    "        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)\n",
    "\n",
    "        m = X.shape[0]\n",
    "        t1f = t1[:, 1:] # threshold values between the input layer and hidden layer (excluding the bias input)\n",
    "        t2f = t2[:, 1:] # threshold values between the hidden layer and output layer (excluding the bias input)\n",
    "        Y = np.eye(num_labels)[y]\n",
    "\n",
    "        Delta1, Delta2 = 0, 0 # initialize matrix Deltas (cost function gradients)\n",
    "        # Iterate over the instances.\n",
    "        for i, row in enumerate(X):\n",
    "            # Forwardprop.\n",
    "            a1, z2, a2, z3, a3 = self._forward(row, t1, t2)\n",
    "\n",
    "            # Backprop.\n",
    "            d3 = a3 - Y[i, :].T # activation error (delta) in the output layer nodes\n",
    "            d2 = np.dot(t2f.T, d3) * self.activation_func_prime(z2) # activation error (delta) in the hidden layer nodes\n",
    "\n",
    "            # Update matrix Deltas (cost function gradients).\n",
    "            Delta2 += np.dot(d3[np.newaxis].T, a2[np.newaxis])\n",
    "            Delta1 += np.dot(d2[np.newaxis].T, a1[np.newaxis])\n",
    "\n",
    "        # The (unregularized) gradients for each theta.\n",
    "        Theta1_grad = (1 / m) * Delta1 \n",
    "        Theta2_grad = (1 / m) * Delta2\n",
    "\n",
    "        # For regularization.\n",
    "        if reg_lambda != 0:\n",
    "            # The regularized gradients for each theta (excluding the bias input).\n",
    "            Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (reg_lambda / m) * t1f\n",
    "            Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (reg_lambda / m) * t2f\n",
    "\n",
    "        return self.pack_thetas(Theta1_grad, Theta2_grad)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model given predictor(s) X and target y.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape=(n, m)\n",
    "            The feature data for which to compute the predicted output.\n",
    "\n",
    "        y : array, shape=(n,1)\n",
    "            The actual outputs (class data).\n",
    "        \"\"\"\n",
    "        num_features = X.shape[0]\n",
    "        input_layer_size = X.shape[1]\n",
    "        num_labels = len(set(y))\n",
    "\n",
    "        # Initialize (random) thetas (threshold values).\n",
    "        theta1_0 = self.rand_init(input_layer_size, self.hidden_layer_size)\n",
    "        theta2_0 = self.rand_init(self.hidden_layer_size, num_labels)\n",
    "        thetas0 = self.pack_thetas(theta1_0, theta2_0)\n",
    "\n",
    "        # Minimize the objective (cost) function and return the resulting thetas.\n",
    "        options = {'maxiter': self.maxiter}\n",
    "        _res = optimize.minimize(self.function, thetas0, jac=self.function_prime, method=self.method, \n",
    "                                 args=(input_layer_size, self.hidden_layer_size, num_labels, X, y, 0), options=options)\n",
    "\n",
    "        # Set the fitted thetas.\n",
    "        self.t1, self.t2 = self.unpack_thetas(_res.x, input_layer_size, self.hidden_layer_size, num_labels)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict labels with the fitted model on predictor(s) X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape=(n, m)\n",
    "            The feature data for which to compute the predicted outputs.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The predicted labels for each instance X.\n",
    "        \"\"\"\n",
    "        return self.predict_proba(X).argmax(0)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict label probabilities with the fitted model on predictor(s) X.\n",
    "\n",
    "        The probabilities are computed as the output activation of units in \n",
    "        the output layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape=(n, m)\n",
    "            The feature data for which to compute the predicted output \n",
    "            probabilities.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        h : array, shape=(n, 1)\n",
    "            The predicted label probabilities for each instance X.\n",
    "        \"\"\"\n",
    "        _, _, _, _, h = self._forward(X, self.t1, self.t2)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MosesAbishekRaj\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "label_encode = True\n",
    "n_folds = 5\n",
    "\n",
    "# Load the Iris flower dataset\n",
    "fileURL = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "iris = pd.read_csv(fileURL, \n",
    "                   names=['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width', 'Species'],\n",
    "                   header=None)\n",
    "iris = iris.dropna()\n",
    "\n",
    "X = np.array(iris[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']]) # features\n",
    "y = iris['Species'] # class\n",
    "\n",
    "if label_encode:\n",
    "    # Transform string (nominal) output to numeric\n",
    "    labels = preprocessing.LabelEncoder().fit_transform(y)\n",
    "else:\n",
    "    labels = y\n",
    "\n",
    "# Generate k stratified folds of the data.\n",
    "skf = list(cross_validation.StratifiedKFold(labels, n_folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-dc896c03c64b>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-dc896c03c64b>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    print '  Fold [%s]' % (i)\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Generate classification objects.\n",
    "nn = NN_1HL()\n",
    "rfc = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "# Generate arrays for meta-level training and testing sets, which are n x len(clfs).\n",
    "scores_nn = np.zeros(n_folds) # scores for nn\n",
    "scores_rfc = np.zeros(n_folds) # scores for rfc\n",
    "\n",
    "print ('Training classifiers...')\n",
    "# Iterate over the folds, each with training set and validation set indicies.\n",
    "for i, (train_index, test_index) in enumerate(skf):\n",
    "    print '  Fold [%s]' % (i)\n",
    "\n",
    "    # Generate the training set for the fold.\n",
    "    X_train = X[train_index]\n",
    "    y_train = labels[train_index]\n",
    "\n",
    "    # Generate the testing set for the fold.\n",
    "    X_test = X[test_index]\n",
    "    y_test = labels[test_index]\n",
    "\n",
    "    # Train the models on the training set.\n",
    "    # We time the training using the built-in timeit magic function.\n",
    "    print '    Neural Network: ', %timeit nn.fit(X_train, y_train)\n",
    "    print '    Random Forest: ', %timeit rfc.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the models on the testing set.\n",
    "    scores_nn[i] = metrics.accuracy_score(y_test, nn.predict(X_test))\n",
    "    scores_rfc[i] = metrics.accuracy_score(y_test, rfc.predict(X_test))\n",
    "print ('Done training classifiers.')\n",
    "\n",
    "# The mean of the scores on the testing set.\n",
    "print ('Artificial Neural Network Accuracy = %s' % (scores_nn.mean(axis=0)))\n",
    "print ('Random Forest Accuracy = %s' % (scores_rfc.mean(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-ae9e8fd8f1ff>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-ae9e8fd8f1ff>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    2to3 chapter-33.py\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    " 2to3 chapter-33.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
