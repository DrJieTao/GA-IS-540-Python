{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MosesAbishekRaj\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\MosesAbishekRaj\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'IterGrid'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e3dc0b9f1853>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#from itertools import izip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_search\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIterGrid\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mClassifierMixin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRegressorMixin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseEnsemble\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'IterGrid'"
     ]
    }
   ],
   "source": [
    "# (c) 2014 Reid Johnson\n",
    "#\n",
    "# Modified from:\n",
    "# Kemal Eren (https://github.com/kemaleren/scikit-learn/blob/stacking/sklearn/ensemble/stacking.py)\n",
    "#\n",
    "# Generates a stacking/blending of base models. Cross-validation is used to \n",
    "# generate predictions from base (level-0) models that are used as input to a \n",
    "# combiner (level-1) model.\n",
    "\n",
    "import numpy as np\n",
    "#from itertools import izip\n",
    "from sklearn.grid_search import IterGrid\n",
    "from sklearn.base import ClassifierMixin, RegressorMixin\n",
    "from sklearn.ensemble.base import BaseEnsemble\n",
    "from sklearn.utils.validation import assert_all_finite\n",
    "\n",
    "# TODO: Built-in nested cross validation, re-using base classifiers, to pick \n",
    "#       best stacking method.\n",
    "# TODO: Access to best, vote, etc. after training.\n",
    "\n",
    "__all__ = [\n",
    "    \"Stacking\",\n",
    "    \"StackingFWL\",\n",
    "    'estimator_grid'\n",
    "]\n",
    "\n",
    "\n",
    "def estimator_grid(*args):\n",
    "    \"\"\"Generate candidate estimators from a list of parameter values on the \n",
    "    combination of the various parameter lists given.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args : array\n",
    "        List of classifiers and corresponding parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : array\n",
    "        The generated estimators.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    pairs = izip(args[::2], args[1::2])\n",
    "    for estimator, params in pairs:\n",
    "        if len(params) == 0:\n",
    "            result.append(estimator())\n",
    "        else:\n",
    "            for p in IterGrid(params):\n",
    "                result.append(estimator(**p))\n",
    "    return result\n",
    "\n",
    "\n",
    "class MRLR(ClassifierMixin):\n",
    "    \"\"\"Converts a multi-class classification task into a set of indicator \n",
    "    regression tasks.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] K. M. Ting, I. H. Witten, \"Issues in Stacked Generalization\", 1999.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, regressor, stackingc, **kwargs):\n",
    "        self.estimator_ = regressor\n",
    "        self.estimator_args_ = kwargs\n",
    "        self.stackingc_ = stackingc\n",
    "\n",
    "    def _get_subdata(self, X):\n",
    "        \"\"\"Returns subsets of the data, one for each class. Assumes the \n",
    "        columns of X are striped in order.\n",
    "\n",
    "        e.g. if n_classes_ == 3, then returns (X[:, 0::3], X[:, 1::3],\n",
    "        X[:, 2::3])\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape=(n, m)\n",
    "            The feature data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array of shape = [len(set(y)), n_samples]\n",
    "            The subsets of the data.\n",
    "        \"\"\"\n",
    "        if not self.stackingc_:\n",
    "            return [X, ] * self.n_classes_\n",
    "\n",
    "        result = []\n",
    "        for i in range(self.n_classes_):\n",
    "            slc = (slice(None), slice(i, None, self.n_classes_))\n",
    "            result.append(X[slc])\n",
    "        return result\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the estimator given predictor(s) X and target y. Assumes the\n",
    "        columns of X are predictions generated by each predictor on each \n",
    "        class. Fits one estimator for each class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape=(n, m)\n",
    "            The feature data for which to compute the predicted output.\n",
    "\n",
    "        y : array of shape = [n_samples]\n",
    "            The actual outputs (class data).\n",
    "        \"\"\"\n",
    "        self.n_classes_ = len(set(y))\n",
    "        self.estimators_ = []\n",
    "\n",
    "        # Generate feature data subsets corresponding to each class.\n",
    "        X_subs = self._get_subdata(X)\n",
    "\n",
    "        # Fit an instance of the estimator to each data subset.\n",
    "        for i in range(self.n_classes_):\n",
    "            e = self.estimator_(**self.estimator_args_)\n",
    "            y_i = np.array(list(j == i for j in y))\n",
    "            X_i = X_subs[i]\n",
    "            e.fit(X_i, y_i)\n",
    "            self.estimators_.append(e)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict label values with the fitted estimator on predictor(s) X.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array of shape = [n_samples]\n",
    "            The predicted label values of the input samples.\n",
    "        \"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        return np.argmax(proba, axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict label probabilities with the fitted estimator on \n",
    "        predictor(s) X.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        proba : array of shape = [n_samples]\n",
    "            The predicted label probabilities of the input samples.\n",
    "        \"\"\"\n",
    "        proba = []\n",
    "\n",
    "        X_subs = self._get_subdata(X)\n",
    "\n",
    "        for i in range(self.n_classes_):\n",
    "            e = self.estimators_[i]\n",
    "            X_i = X_subs[i]\n",
    "            pred = e.predict(X_i).reshape(-1, 1)\n",
    "            proba.append(pred)\n",
    "        proba = np.hstack(proba)\n",
    "\n",
    "        normalizer = proba.sum(axis=1)[:, np.newaxis]\n",
    "        normalizer[normalizer == 0.0] = 1.0\n",
    "        proba /= normalizer\n",
    "\n",
    "        assert_all_finite(proba)\n",
    "\n",
    "        return proba\n",
    "\n",
    "\n",
    "class Stacking(BaseEnsemble):\n",
    "    \"\"\"Implements stacking/blending.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    meta_estimator : string or callable\n",
    "        May be one of \"best\", \"vote\", \"average\", or any classifier or \n",
    "        regressor constructor\n",
    "\n",
    "    estimators : iterator\n",
    "        An iterable of estimators; each must support predict_proba()\n",
    "\n",
    "    cv : iterator\n",
    "        A cross validation object. Base (level-0) estimators are trained on \n",
    "        the training folds, then the meta (level-1) estimator is trained on \n",
    "        the testing folds.\n",
    "\n",
    "    stackingc : bool\n",
    "        Whether to use StackingC or not. For more information, refer to the \n",
    "        following paper:\n",
    "\n",
    "        Reference:\n",
    "          A. K. Seewald, \"How to Make Stacking Better and Faster While Also \n",
    "          Taking Care of an Unknown Weakness,\" 2002.\n",
    "\n",
    "    kwargs :\n",
    "        Arguments passed to instantiate meta_estimator.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] D. H. Wolpert, \"Stacked Generalization\", 1992.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Support different features for each estimator.\n",
    "    # TODO: Support \"best\", \"vote\", and \"average\" for already trained model.\n",
    "    # TODO: Allow saving of estimators, so they need not be retrained when \n",
    "    #       trying new stacking methods.\n",
    "\n",
    "    def __init__(self, meta_estimator, estimators,\n",
    "                 cv, stackingc=True, proba=True,\n",
    "                 **kwargs):\n",
    "        self.estimators_ = estimators\n",
    "        self.n_estimators_ = len(estimators)\n",
    "        self.cv_ = cv\n",
    "        self.stackingc_ = stackingc\n",
    "        self.proba_ = proba\n",
    "\n",
    "        if stackingc:\n",
    "            if isinstance(meta_estimator, str) or not issubclass(meta_estimator, RegressorMixin):\n",
    "                raise Exception('StackingC only works with a regressor.')\n",
    "\n",
    "        if isinstance(meta_estimator, str):\n",
    "            if meta_estimator not in ('best',\n",
    "                                      'average',\n",
    "                                      'vote'):\n",
    "                raise Exception('Invalid meta estimator: {0}'.format(meta_estimator))\n",
    "            raise Exception('\"{0}\" meta estimator not implemented'.format(meta_estimator))\n",
    "        elif issubclass(meta_estimator, ClassifierMixin):\n",
    "            self.meta_estimator_ = meta_estimator(**kwargs)\n",
    "        elif issubclass(meta_estimator, RegressorMixin):\n",
    "            self.meta_estimator_ = MRLR(meta_estimator, stackingc, **kwargs)\n",
    "        else:\n",
    "            raise Exception('Invalid meta estimator: {0}'.format(meta_estimator))\n",
    "\n",
    "    def _base_estimator_predict(self, e, X):\n",
    "        \"\"\"Predict label values with the specified estimator on predictor(s) X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        e : int\n",
    "            The estimator object.\n",
    "\n",
    "        X : np.ndarray, shape=(n, m)\n",
    "            The feature data for which to compute the predicted outputs.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pred : np.ndarray, shape=(len(X), 1)\n",
    "            The mean of the label probabilities predicted by the specified \n",
    "            estimator for each fold for each instance X.\n",
    "        \"\"\"\n",
    "        # Generate array for the base-level testing set, which is n x n_folds.\n",
    "        pred = e.predict(X)\n",
    "        assert_all_finite(pred)\n",
    "        return pred\n",
    "\n",
    "    def _base_estimator_predict_proba(self, e, X):\n",
    "        \"\"\"Predict label probabilities with the specified estimator on \n",
    "        predictor(s) X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        e : int\n",
    "            The estimator object.\n",
    "\n",
    "        X : np.ndarray, shape=(n, m)\n",
    "            The feature data for which to compute the predicted outputs.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pred : np.ndarray, shape=(len(X), 1)\n",
    "            The mean of the label probabilities predicted by the specified \n",
    "            estimator for each fold for each instance X.\n",
    "        \"\"\"\n",
    "        # Generate array for the base-level testing set, which is n x n_folds.\n",
    "        pred = e.predict_proba(X)\n",
    "        assert_all_finite(pred)\n",
    "        return pred\n",
    "\n",
    "    def _make_meta(self, X):\n",
    "        \"\"\"Make the feature set for the meta (level-1) estimator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape=(n, m)\n",
    "            The feature data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        An n x len(self.estimators_) array of meta-level features.\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        for e in self.estimators_:\n",
    "            if self.proba_:\n",
    "                # Predict label probabilities\n",
    "                pred = self._base_estimator_predict_proba(e, X)\n",
    "            else:\n",
    "                # Predict label values\n",
    "                pred = self._base_estimator_predict(e, X)\n",
    "            rows.append(pred)\n",
    "        return np.hstack(rows)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the estimator given predictor(s) X and target y.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape=(n, m)\n",
    "            The feature data on which to fit.\n",
    "\n",
    "        y : array of shape = [n_samples]\n",
    "            The actual outputs (class data).\n",
    "        \"\"\"\n",
    "        # Build meta data.\n",
    "        X_meta = [] # meta-level features\n",
    "        y_meta = [] # meta-level labels\n",
    "\n",
    "        print( 'Training and validating the base (level-0) estimator(s)...')\n",
    "        print\n",
    "        for i, (a, b) in enumerate(self.cv_):\n",
    "            print ('Fold [%s]' % (i))\n",
    "\n",
    "            X_a, X_b = X[a], X[b] # training and validation features\n",
    "            y_a, y_b = y[a], y[b] # training and validation labels\n",
    "\n",
    "            # Fit each base estimator using the training set for the fold.\n",
    "            for j, e in enumerate(self.estimators_):\n",
    "                print ('  Training base (level-0) estimator %d...' % (j),)\n",
    "                e.fit(X_a, y_a)\n",
    "                print ('done.')\n",
    "\n",
    "            proba = self._make_meta(X_b)\n",
    "            X_meta.append(proba)\n",
    "            y_meta.append(y_b)\n",
    "        \n",
    "\n",
    "        X_meta = np.vstack(X_meta)\n",
    "        if y_meta[0].ndim == 1:\n",
    "            y_meta = np.hstack(y_meta)\n",
    "        else:\n",
    "            y_meta = np.vstack(y_meta)\n",
    "\n",
    "        # Train meta estimator.\n",
    "        print ('Training meta (level-1) estimator...',)\n",
    "        self.meta_estimator_.fit(X_meta, y_meta)\n",
    "        print ('done.')\n",
    "\n",
    "        # Re-train base estimators on full data.\n",
    "        for j, e in enumerate(self.estimators_):\n",
    "            print ('Re-training base (level-0) estimator %d on full data...' % (j),)\n",
    "            e.fit(X, y)\n",
    "            print ('done.')\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict label values with the fitted estimator on predictor(s) X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape=(n, m)\n",
    "            The feature data for which to compute the predicted output.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array of shape = [n_samples]\n",
    "            The predicted label values of the input samples.\n",
    "        \"\"\"\n",
    "        X_meta = self._make_meta(X)\n",
    "        return self.meta_estimator_.predict(X_meta)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict label probabilities with the fitted estimator on \n",
    "        predictor(s) X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape=(n, m)\n",
    "            The feature data for which to compute the predicted output.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array of shape = [n_samples]\n",
    "            The predicted label probabilities of the input samples.\n",
    "        \"\"\"\n",
    "        X_meta = self._make_meta(X)\n",
    "        return self.meta_estimator_.predict_proba(X_meta)\n",
    "\n",
    "\n",
    "class StackingFWL(Stacking):\n",
    "    \"\"\"Implements Feature-Weighted Linear Stacking.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] J. Sill, G. Takács, L. Mackey, D. Lin, \"Feature-Weighted Linear \n",
    "           Stacking\", 2009.\n",
    "\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
